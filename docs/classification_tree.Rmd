---
title: "Classification_Decision_Tree"
author: "Nivedita"
date: "2024-04-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objective

To fit classification decision tree for Carseats data set in ISLR2 package.

## Analysis

```{r echo=TRUE}
library(ISLR2)
attach(Carseats)
```

Carseats is a simulated data set containing sales of child car seats at 400 different stores. It is a data frame with 400 observations on the following 11 variables. The variables are as follows:

Sales-Unit sales (in thousands) at each location

CompPrice-Price charged by competitor at each location

Income-Community income level (in thousands of dollars)

Advertising-Local advertising budget for company at each location (in thousands of dollars)

Population-Population size in region (in thousands)

Price-Price company charges for car seats at each site

ShelveLoc-A factor with levels Bad, Good and Medium indicating the quality of the shelving location for the car seats at each site

Age-Average age of the local population

Education-Education level at each location

Urban-A factor with levels No and Yes to indicate whether the store is in an urban or rural location

US-A factor with levels No and Yes to indicate whether the store is in the US or not


We want to fit classification tree for this data set. For this first we create a variable, called High, which takes on a value of Yes if the Sales variable exceeds 8, and takes on a value of No otherwise.

```{r echo=TRUE}
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#to merge High with the rest of the Carseats data.
Carseats <- data.frame(Carseats, High)
```

We now use the tree() function to fit a classification tree in order to predict High using all variables except sales.

```{r echo=TRUE}
library(tree)
tree.carseats <- tree(High ~ .- Sales, Carseats)
summary(tree.carseats)
```

From summary we can see that "ShelveLoc", "Price", "Income", "Population", "Advertising", "Age","CompPrice" and "US" are the variables which are used in tree construction. There are 27 terminal nodes and residual mean deviance is 0.4575. We see that the training error rate is 9%.
 
Now, for visual representation we use the plot() function to display the tree structure and the text() function to display the node labels.

```{r echo=TRUE}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

*Fig.1 Plot of classification tree structure*

In order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error. We split the observations into a training set and a test set, build the tree using the training set, and evaluate its performance on the test data.

```{r echo=TRUE}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train, ]
High.test <- High[-train]
tree.carseats <- tree(High ~ .- Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
(104 + 50) / 200
```

We can see that this approach leads to correct predictions for around 77% of the locations in the test data set. To observe the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k) we performed-

```{r echo=TRUE}
set.seed(7)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
cv.carseats
```

We can see that the tree with 9 terminal nodes results in only 74 cross-validation errors. Now we plot the error rate as a function of both size and k.

```{r echo=TRUE}
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b",main='Plot of error rate function of size' )
plot(cv.carseats$k, cv.carseats$dev, type = "b",main='Plot of error rate function of k')
```

*Fig. 2 Plot of error rate function of size and k*

From Fig. 2 we can see that 9 terminal nodes results minnimum cross validation errors.
We now want to check whether pruning the tree might lead to improved
 results so we apply the prune.misclass() function in order to prune the tree to obtain the nine-node tree.

```{r echo=TRUE}
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

*Fig. 3 Plot of prune tree*

Now we test how does this prune tree perform on test data.

```{r echo=TRUE}
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
High.test
(97 + 58) / 200
```

Now 77.5% of the test observations are correctly classified, so not only has the pruning process produced a more interpretable tree, but it has also slightly improved the classification accuracy.

### Bagging

Here we apply bagging to the Carseats data, using the randomForest package in R. The argument mtry = 10 indicates that all 10 predictors should be considered for each split of the tree—in other words, that bagging should be done.

```{r include=FALSE}
library(randomForest)
```
```{r echo=TRUE}
set.seed(1)
bag.Carseats <- randomForest(High ~ .- Sales, data = Carseats,subset = train, mtry = 10, importance = TRUE)
bag.Carseats
```

Now to check how well does this bagged model perform on the test set.

```{r echo=TRUE}
yhat.bag <- predict(bag.Carseats, Carseats.test, type = "class")
b.table<-table(yhat.bag,High.test)
b.table
(104+61)/(104+22+13+61)
```
Now from bagging 82.5% of the test observations are correctly classified which is more than that we have obtained using an optimally-pruned single tree.

### Random Forest

Here we apply random forest to the Carseats data, using same randomForest package in R. Now we use √p variables when building a random forest of classification trees. Here we use mtry=3.

```{r echo=TRUE}
set.seed(1)
rf.Carseats <-randomForest(High ~ .- Sales, data = Carseats, subset = train, mtry = 3, importance = TRUE)
yhat.rf <-predict(rf.Carseats, Carseats.test, type = "class")
r.table<-table(yhat.rf,High.test)
r.table
(110+59)/(110+59+7+24)
```
We can see that from random forest 84.5% of the test observations are correctly classified which is more than bagging.


We can use the importance() function, to view the importance of each variable. From this function two measures of variable importance are reported. The first is based upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is permuted. The second is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees.

```{r echo=TRUE}
# to check importance
importance(rf.Carseats)
#plot
varImpPlot(rf.Carseats,main = 'Plots of importance measures')
```

*Fig. 4 Plots of importance measures*

From Fig. 4 we can see that across all of the trees considered in the random forest, the Price company charges for car seats at each site (Price) is most important variables.

### Boosting

Here we use the gbm package, and within it the gbm() function, to fit boosted gbm() regression trees to the Boston data set.

```{r echo=TRUE}
library(gbm)
set.seed(1)
#to convert into dummy variable
d1<-data.frame(na.omit(Carseats[train,c('US', "High",'Urban') ]),stringsAsFactors=FALSE)
dyn<-ifelse(d1 == "Yes",1,0)
c.train<-Carseats[train, ]
boost.d1<-c.train[,-c(10:12)]
boost.d2<-boost.d1[,-7]
carseat.d<-data.frame(boost.d2,dyn)

boost.Carseats <- gbm(High ~ .- Sales, data = carseat.d, distribution = "bernoulli", n.trees = 5000, interaction.depth = 4)
summary(boost.Carseats)

```

From summary we can see that Price is by far the most important variables. We can also produce partial dependence plots for this variable

```{r echo=TRUE}
plot(boost.Carseats, i = "Price", main='Partial dependence plot of Price')
```

*Fig. 5 Partial dependence plot of Price*


## Conclusion


We performed fitting of classification tree, bagging, boosting and random forest for Carseats data set in ISLR2 package. We observed that classification accuracy of random forest is maximum and Price is the most important variable.

 